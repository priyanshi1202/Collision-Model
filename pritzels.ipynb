{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15525748-bab8-43a3-9c8e-49ab319d0f41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming you have TLE data files for each debris source as CSV files\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have TLE data files for each debris source as CSV files\n",
    "iridium_33 = pd.read_csv(r\"iridium_33.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575dfbc-980a-4e41-a7e7-96a7c7bce181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_1408 = pd.read_csv(r\"cosmos_1408.csv\")\n",
    "fengyun_1c = pd.read_csv(r\"fengyun_1c.csv\")\n",
    "iridium_33 = pd.read_csv(r\"iridium_33.csv\")\n",
    "cosmos_2251 = pd.read_csv(r\"cosmos_2251.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e359636-519c-4794-83fb-a87a148d7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Semimajor Axis from Mean Motion\n",
    "def calculate_semimajor_axis(mean_motion):\n",
    "    MU = 398600.4418  # Gravitational parameter, km^3/s^2\n",
    "    n = mean_motion * 2 * np.pi / 86400  # Convert from rev/day to rad/sec\n",
    "    semimajor_axis = (MU / (n ** 2)) ** (1/3)\n",
    "    return semimajor_axis\n",
    "\n",
    "# Add Semimajor Axis to each dataset\n",
    "for df in [cosmos_1408, fengyun_1c, iridium_33, cosmos_2251]:\n",
    "    df['SEMIMAJOR_AXIS'] = df['MEAN_MOTION'].apply(calculate_semimajor_axis)\n",
    "\n",
    "# Optional: Add a column to label the debris source\n",
    "cosmos_1408['DEBRIS_TYPE'] = 'COSMOS 1408'\n",
    "fengyun_1c['DEBRIS_TYPE'] = 'FENGYUN 1C'\n",
    "iridium_33['DEBRIS_TYPE'] = 'IRIDIUM 33'\n",
    "cosmos_2251['DEBRIS_TYPE'] = 'COSMOS 2251'\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_debris_data = pd.concat([cosmos_1408, fengyun_1c, iridium_33, cosmos_2251], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_debris_data.to_csv(\"combined_debris_data.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "print(combined_debris_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5ad87-f398-435d-bf9e-97ab5821984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate semimajor axis from mean motion\n",
    "def calculate_semimajor_axis(mean_motion):\n",
    "    \"\"\"\n",
    "    Calculate semimajor axis (in km) from mean motion (in rev/day).\n",
    "    \"\"\"\n",
    "    MU = 398600.4418  # Earth's gravitational parameter (km^3/s^2)\n",
    "    n = mean_motion * 2 * np.pi / 86400  # Convert rev/day to rad/s\n",
    "    semimajor_axis = (MU / (n ** 2)) ** (1 / 3)  # Kepler's Third Law\n",
    "    return semimajor_axis\n",
    "\n",
    "# Function to process a TLE dataset\n",
    "def process_tle_dataset(file_path, satellite_type):\n",
    "    \"\"\"\n",
    "    Process a TLE dataset:\n",
    "    - Load TLE data.\n",
    "    - Calculate semimajor axis.\n",
    "    - Add a column for satellite type.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure 'MEAN_MOTION' exists and calculate semimajor axis\n",
    "    if 'MEAN_MOTION' in data.columns:\n",
    "        data['SEMIMAJOR_AXIS'] = data['MEAN_MOTION'].apply(calculate_semimajor_axis)\n",
    "    else:\n",
    "        print(f\"Error: MEAN_MOTION column missing in {file_path}.\")\n",
    "        return None\n",
    "\n",
    "    # Add satellite type column\n",
    "    data['SATELLITE_TYPE'] = satellite_type\n",
    "\n",
    "    return data\n",
    "\n",
    "# Paths to the TLE datasets\n",
    "cubesat_path = r\"cubesat.csv\"\n",
    "iridium_next_path = r\"iridium_next.csv\"\n",
    "globalstar_path =r\"globalstar.csv\"\n",
    "orbcomm_path = r\"orbcomm.csv\"\n",
    "amateur_radio_path = r\"amateur_radio.csv\"\n",
    "\n",
    "# Process each dataset\n",
    "cubesat_data = process_tle_dataset(cubesat_path, \"CubeSat\")\n",
    "iridium_next_data = process_tle_dataset(iridium_next_path, \"Iridium Next\")\n",
    "globalstar_data = process_tle_dataset(globalstar_path, \"Globalstar\")\n",
    "orbcomm_data = process_tle_dataset(orbcomm_path, \"Orbcomm\")\n",
    "amateur_radio_data = process_tle_dataset(amateur_radio_path, \"Amateur Radio\")\n",
    "\n",
    "# Combine all datasets\n",
    "all_satellites_data = pd.concat([cubesat_data, iridium_next_data, globalstar_data, orbcomm_data, amateur_radio_data], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "all_satellites_data.to_csv(\"merged_satellite_data.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as 'merged_satellite_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58598c98-1514-42cf-9c56-96db54c867b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_satellites_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd22c4-f526-4db9-8666-f217b9c57453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the merged dataset\n",
    "data = pd.read_csv(\"merged_satellite_data.csv\")\n",
    "\n",
    "# Separate CubeSat and Non-CubeSat entries\n",
    "cubesat_data = data[data['SATELLITE_TYPE'] == \"CubeSat\"]\n",
    "non_cubesat_data = data[data['SATELLITE_TYPE'] != \"CubeSat\"]\n",
    "\n",
    "print(f\"CubeSat entries: {len(cubesat_data)}\")\n",
    "print(f\"Non-CubeSat entries: {len(non_cubesat_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3296d-0d1c-4408-8d75-df7bf4b104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample CubeSat data to 250\n",
    "cubesat_oversampled = resample(cubesat_data,\n",
    "                               replace=True,  # Allow replacement\n",
    "                               n_samples=250,  # Desired size\n",
    "                               random_state=42)\n",
    "\n",
    "# Downsample Non-CubeSat data to 250\n",
    "non_cubesat_downsampled = resample(non_cubesat_data,\n",
    "                                   replace=False,  # No replacement\n",
    "                                   n_samples=250,  # Desired size\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine oversampled CubeSat and downsampled Non-CubeSat data\n",
    "balanced_data = pd.concat([cubesat_oversampled, non_cubesat_downsampled])\n",
    "\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e341e7-2034-4c29-9ac9-442167902872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca5807-28d3-4008-9a52-1b17d738e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset after removing duplicates: {balanced_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528757c-bb38-4ae8-bc04-9e2dd5773e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Balanced dataset size: {len(balanced_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e0394-327b-49a9-a26e-2dc339d71c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_data['SATELLITE_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955081a-671c-4660-96b5-1e7d275f8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate each class into its own DataFrame\n",
    "cubesat_data = balanced_data[balanced_data['SATELLITE_TYPE'] == 'CubeSat']\n",
    "globalstar_data = balanced_data[balanced_data['SATELLITE_TYPE'] == 'Globalstar']\n",
    "amateur_radio_data = balanced_data[balanced_data['SATELLITE_TYPE'] == 'Amateur Radio']\n",
    "iridium_next_data = balanced_data[balanced_data['SATELLITE_TYPE'] == 'Iridium Next']\n",
    "orbcomm_data = balanced_data[balanced_data['SATELLITE_TYPE'] == 'Orbcomm']\n",
    "\n",
    "# Target sizes for each class\n",
    "target_cubesat = int(len(balanced_data) * 0.5)  # 50% of the dataset\n",
    "target_other = int(len(balanced_data) * 0.5 / 4)  # Split remaining 50% among other types\n",
    "\n",
    "# Oversample CubeSat data\n",
    "cubesat_oversampled = resample(cubesat_data, replace=True, n_samples=target_cubesat, random_state=42)\n",
    "\n",
    "# Downsample other satellite types\n",
    "globalstar_downsampled = resample(globalstar_data, replace=False, n_samples=target_other, random_state=42)\n",
    "amateur_radio_downsampled = resample(amateur_radio_data, replace=False, n_samples=target_other, random_state=42)\n",
    "iridium_next_downsampled = resample(iridium_next_data, replace=False, n_samples=target_other, random_state=42)\n",
    "orbcomm_downsampled = resample(orbcomm_data, replace=False, n_samples=target_other, random_state=42)\n",
    "\n",
    "# Combine the balanced dataset\n",
    "focused_data = pd.concat([\n",
    "    cubesat_oversampled,\n",
    "    globalstar_downsampled,\n",
    "    amateur_radio_downsampled,\n",
    "    iridium_next_downsampled,\n",
    "    orbcomm_downsampled\n",
    "])\n",
    "\n",
    "# Shuffle the dataset\n",
    "focused_data = focused_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(focused_data['SATELLITE_TYPE'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af066e58-d5c4-4581-86ca-c16742c30a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total dataset size: {len(focused_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021b9bc-b9d7-4e74-ae53-22bdc1c25bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates remaining: {focused_data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a13a5a-8ffb-4ae3-9d19-5dbd93d31469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates remaining: {focused_data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f682d-d043-464f-81a1-be994a2b6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_data.to_csv(\"focused_satellite_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e5f8a-d9d4-4463-994b-1d1cdcdb8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = focused_data[focused_data.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80ea9d-972e-4ce1-bddf-8eca1db75aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = focused_data[focused_data.duplicated()]\n",
    "\n",
    "# Check columns with differences among duplicates\n",
    "duplicate_columns = []\n",
    "for column in focused_data.columns:\n",
    "    if not focused_data[column].duplicated().all():\n",
    "        duplicate_columns.append(column)\n",
    "\n",
    "print(\"Columns with duplicates:\", duplicate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0609e8-a466-4d02-b251-5b4eed6fe336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows where all columns are identical\n",
    "duplicates = focused_data[focused_data.duplicated()]\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d0c62-e28b-4036-9cde-e8256e5bf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns to check for duplicates\n",
    "columns_to_check = ['SEMIMAJOR_AXIS', 'ECCENTRICITY', 'BSTAR', 'INCLINATION']\n",
    "duplicates = focused_data[focused_data.duplicated(subset=columns_to_check)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90834573-6d95-412d-a99b-9f68813c46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicates based on important columns (like OBJECT_NAME, OBJECT_ID, NORAD_CAT_ID, SEMIMAJOR_AXIS)\n",
    "key_columns = ['OBJECT_NAME', 'OBJECT_ID', 'NORAD_CAT_ID']\n",
    "duplicates_key_columns = focused_data[focused_data.duplicated(subset=key_columns)]\n",
    "print(duplicates_key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c323da3-0686-4f95-a77d-46ad5cb8df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "mu_earth = 398600  # Gravitational parameter for Earth in km^3/s^2\n",
    "earth_radius = 6371  # Earth's radius in km\n",
    "\n",
    "# 1. Calculate Orbital Period (T)\n",
    "# Orbital Period in seconds\n",
    "focused_data['ORBITAL_PERIOD'] = 2 * np.pi * np.sqrt(focused_data['SEMIMAJOR_AXIS']**3 / mu_earth)\n",
    "\n",
    "# Convert to hours (optional)\n",
    "focused_data['ORBITAL_PERIOD_HOURS'] = focused_data['ORBITAL_PERIOD'] / 3600\n",
    "\n",
    "# 2. Calculate Altitude\n",
    "# Altitude in km (distance from Earth's surface)\n",
    "focused_data['ALTITUDE'] = focused_data['SEMIMAJOR_AXIS'] - earth_radius\n",
    "\n",
    "# 3. Calculate Perigee and Apogee\n",
    "# Perigee = a(1 - e), Apogee = a(1 + e)\n",
    "focused_data['PERIGEE'] = focused_data['SEMIMAJOR_AXIS'] * (1 - focused_data['ECCENTRICITY'])\n",
    "focused_data['APOGEE'] = focused_data['SEMIMAJOR_AXIS'] * (1 + focused_data['ECCENTRICITY'])\n",
    "\n",
    "# Optionally, you can print the first few rows to verify the calculations\n",
    "print(focused_data[['OBJECT_NAME', 'SEMIMAJOR_AXIS', 'ECCENTRICITY', 'ORBITAL_PERIOD', 'ALTITUDE', 'PERIGEE', 'APOGEE']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce4d76-42d0-4d63-bca4-0db68a11992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = focused_data[focused_data.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa836d-0df5-4c2e-aa62-03025ce8e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesats = focused_data[focused_data['SATELLITE_TYPE'].str.contains(\"CubeSat\", case=False, na=False)]\n",
    "cubesats.to_csv(\"cubesats1_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2ef52-a622-4b13-8544-60d73f81a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c958c-8286-49b6-af68-0786128eba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_debris = pd.read_csv(r\"C:\\Users\\rajve\\collision_course\\debris\\combined1_debris_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb55a98-4db5-459f-8c50-37599c41da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecb29c-3fa8-4113-b6a4-01f3f855bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_debris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e017685-f667-4278-9dd1-a4b1ec3e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the object type for CubeSats\n",
    "cubesats['OBJECT_TYPE'] = cubesats['SATELLITE_TYPE']\n",
    "\n",
    "# Step 2: Standardize the object type for debris\n",
    "all_debris['OBJECT_TYPE'] = all_debris['DEBRIS_TYPE']\n",
    "\n",
    "# Step 3: Drop the old type columns (optional)\n",
    "cubesats.drop(columns=['SATELLITE_TYPE'], inplace=True)\n",
    "all_debris.drop(columns=['DEBRIS_TYPE'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02f75c-40b8-4847-8699-2fed263b754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365a2e4-90be-449f-a5ac-f6d53f612e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_debris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0055d12-4e30-4ff2-b6da-a5ce14b0495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a key column for cross-joining\n",
    "cubesats['key'] = 1\n",
    "all_debris['key'] = 1\n",
    "\n",
    "# Generate all pairs of CubeSats and debris\n",
    "pair_data = pd.merge(cubesats, all_debris, on=\"key\").drop(columns=\"key\")\n",
    "\n",
    "# Verify the output\n",
    "print(pair_data.head())\n",
    "print(f\"Total pairs generated: {len(pair_data)}\")\n",
    "\n",
    "# Save the pairs for further processing\n",
    "pair_data.to_csv(\"cubesat_debris_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8850dc7-c441-40df-8b27-fb2bb37bcc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c068f-3e4d-4c4e-9f5f-8dfe217d3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = list(pair_data.columns)\n",
    "print(column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31135dc-09c0-425d-b26c-ffdca7373878",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row = pair_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a920140-1dcb-45f3-bc65-176a62cea300",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2806b-cad5-4eef-9a8e-0fa3bcaf220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data['EPOCH_x'] = pd.to_datetime(pair_data['EPOCH_x'], errors='coerce')\n",
    "pair_data['EPOCH_y'] = pd.to_datetime(pair_data['EPOCH_y'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4a8e1-4bdb-4677-aebf-01bb7b0a7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "mu_earth = 398600  # Gravitational parameter for Earth (km^3/s^2)\n",
    "earth_radius = 6371  # Earth's radius (km)\n",
    "\n",
    "# 1. Relative Altitude\n",
    "pair_data['RELATIVE_ALTITUDE'] = np.abs(pair_data['ALTITUDE_x'] - pair_data['ALTITUDE_y'])\n",
    "\n",
    "# 2. Relative Inclination\n",
    "pair_data['RELATIVE_INCLINATION'] = np.abs(pair_data['INCLINATION_x'] - pair_data['INCLINATION_y'])\n",
    "\n",
    "# 3. Relative Velocity (using mean motion difference)\n",
    "pair_data['RELATIVE_VELOCITY'] = np.abs(pair_data['MEAN_MOTION_x'] - pair_data['MEAN_MOTION_y'])\n",
    "\n",
    "# 4. Distance Approximation (3D geometry)\n",
    "pair_data['DISTANCE'] = np.sqrt(\n",
    "    pair_data['RELATIVE_ALTITUDE']**2 + (pair_data['RELATIVE_INCLINATION'] * np.pi / 180)**2\n",
    ")\n",
    "\n",
    "# 5. Orbital Overlap (simplified estimate, assuming proximity in inclination and RAAN)\n",
    "pair_data['ORBITAL_OVERLAP'] = np.where(\n",
    "    (pair_data['RELATIVE_INCLINATION'] < 1) & (np.abs(pair_data['RA_OF_ASC_NODE_x'] - pair_data['RA_OF_ASC_NODE_y']) < 10),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "# 6. Perigee and Apogee Difference\n",
    "pair_data['PERIGEE_DIFF'] = np.abs(pair_data['PERIGEE_x'] - pair_data['PERIGEE_y'])\n",
    "pair_data['APOGEE_DIFF'] = np.abs(pair_data['APOGEE_x'] - pair_data['APOGEE_y'])\n",
    "\n",
    "# 7. Orbital Period (in hours)\n",
    "pair_data['ORBITAL_PERIOD_x'] = 2 * np.pi * np.sqrt(pair_data['SEMIMAJOR_AXIS_x']**3 / mu_earth)\n",
    "pair_data['ORBITAL_PERIOD_y'] = 2 * np.pi * np.sqrt(pair_data['SEMIMAJOR_AXIS_y']**3 / mu_earth)\n",
    "\n",
    "# 8. Epoch Difference (in seconds)\n",
    "pair_data['EPOCH_DIFF'] = np.abs(pair_data['EPOCH_x'] - pair_data['EPOCH_y'])\n",
    "\n",
    "# 9. RAAN Difference\n",
    "pair_data['RAAN_DIFF'] = np.abs(pair_data['RA_OF_ASC_NODE_x'] - pair_data['RA_OF_ASC_NODE_y'])\n",
    "\n",
    "# Check the new columns\n",
    "print(pair_data[['RELATIVE_ALTITUDE', 'RELATIVE_INCLINATION', 'RELATIVE_VELOCITY', 'DISTANCE', \n",
    "                 'ORBITAL_OVERLAP', 'PERIGEE_DIFF', 'APOGEE_DIFF', 'EPOCH_DIFF', 'RAAN_DIFF']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e435b1-6934-4b6d-b0c2-a0606387bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = list(pair_data.columns)\n",
    "print(column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8a64d-84a1-4143-82be-bef433efb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert EPOCH_DIFF to seconds (or another unit if needed)\n",
    "pair_data['EPOCH_DIFF'] = pair_data['EPOCH_DIFF'].dt.total_seconds()\n",
    "\n",
    "# Scale the relevant features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "features_to_scale = ['DISTANCE', 'RELATIVE_VELOCITY', 'RELATIVE_ALTITUDE', \n",
    "                     'RELATIVE_INCLINATION', 'PERIGEE_DIFF', 'APOGEE_DIFF', \n",
    "                     'EPOCH_DIFF']\n",
    "\n",
    "pair_data[features_to_scale] = scaler.fit_transform(pair_data[features_to_scale])\n",
    "\n",
    "# Calculate RISK_SCORE\n",
    "pair_data['RISK_SCORE'] = (\n",
    "    0.3 * pair_data['DISTANCE'] +\n",
    "    0.3 * pair_data['RELATIVE_VELOCITY'] +\n",
    "    0.2 * pair_data['ORBITAL_OVERLAP'] +\n",
    "    0.1 * pair_data['RELATIVE_ALTITUDE'] +\n",
    "    0.05 * pair_data['RELATIVE_INCLINATION'] +\n",
    "    0.05 * pair_data['PERIGEE_DIFF'] +\n",
    "    0.05 * pair_data['APOGEE_DIFF']\n",
    ")\n",
    "\n",
    "# Assign a COLLISION_RISK based on the median risk score\n",
    "median_risk_score = pair_data['RISK_SCORE'].median()\n",
    "pair_data['COLLISION_RISK'] = np.where(pair_data['RISK_SCORE'] > median_risk_score, 1, 0)\n",
    "\n",
    "# Print the resulting data\n",
    "print(pair_data[['RISK_SCORE', 'COLLISION_RISK']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b94e96-4d59-4fe2-a845-a0efc33bbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bb986-1e1c-4afb-a440-9526ea0f023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where COLLISION_RISK is 1\n",
    "count_risk_1 = pair_data['COLLISION_RISK'].sum()\n",
    "\n",
    "print(\"Number of rows with RISK_SCORE > median (COLLISION_RISK = 1):\", count_risk_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ab99f-22f9-4ef2-aaa9-e3d3bb3d75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data.to_csv(\"pair_data_with_target.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92646143-17f9-4c88-9211-5ad81895e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = pair_data[[\"RELATIVE_ALTITUDE\", \"RELATIVE_VELOCITY\", \"DISTANCE\", \"ORBITAL_OVERLAP\"]]  # Add other relevant features\n",
    "y = pair_data['COLLISION_RISK']  # Binary target variable (collision risk)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the train and test sets\n",
    "print(f\"Training set size: {X_train.shape}, Testing set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03729eee-3107-4576-a626-e7ddc784bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print out some predictions\n",
    "print(\"Predictions on Test Set:\", y_pred[:10])  # Print first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec6f94-42e7-4825-9a6e-1ad9a2db5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10_risk_scores = pair_data['COLLISION_RISK'].head(10)\n",
    "\n",
    "# Print the first 10 RISK_SCORE values\n",
    "print(first_10_risk_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788c0a0-80a9-4b5b-8955-12bfbaf20d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Evaluate model performance on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05c928-6f32-4ea6-92ba-b87b08dadd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
